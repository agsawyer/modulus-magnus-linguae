At the start of the project, we wanted to count how different languages were tokenized in order to get a feel for how well the model understood the language. This is because if the words are less broken down, ie the model is tokenizing entire words, that means the model has less of an "understanding" of how the language works. 